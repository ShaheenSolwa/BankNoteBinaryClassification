{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VWTI</th>\n",
       "      <th>SWTI</th>\n",
       "      <th>CWTI</th>\n",
       "      <th>EI</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.2634</td>\n",
       "      <td>-4.4862</td>\n",
       "      <td>3.6558</td>\n",
       "      <td>-0.612510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.2718</td>\n",
       "      <td>1.7837</td>\n",
       "      <td>2.1161</td>\n",
       "      <td>0.613340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.9411</td>\n",
       "      <td>-12.8792</td>\n",
       "      <td>13.0597</td>\n",
       "      <td>-3.312500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5195</td>\n",
       "      <td>-3.2633</td>\n",
       "      <td>3.0895</td>\n",
       "      <td>-0.984900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.5698</td>\n",
       "      <td>-4.4076</td>\n",
       "      <td>5.9856</td>\n",
       "      <td>0.078002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     VWTI     SWTI     CWTI        EI  Class\n",
       "0  2.2634  -4.4862   3.6558 -0.612510      0\n",
       "1  3.2718   1.7837   2.1161  0.613340      0\n",
       "2 -3.9411 -12.8792  13.0597 -3.312500      1\n",
       "3  0.5195  -3.2633   3.0895 -0.984900      0\n",
       "4  2.5698  -4.4076   5.9856  0.078002      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bank_note_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Datasets/master/bank_note_data/training_set_label.csv\" )\n",
    "\n",
    "bank_note_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = bank_note_data['Class']\n",
    "featureCols = ['VWTI', 'SWTI', 'CWTI', 'EI']\n",
    "X = bank_note_data[featureCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                test_size=0.33, \n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred=logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9861878453038674\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.4475138121547\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/bank_note_data/testing_set_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTest = logreg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YTEst = model.predict(test_data)\n",
    "YTEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = bank_note_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68382391, 0.35040542, 0.38520258, 0.72157724],\n",
       "       [0.7579726 , 0.58697596, 0.31887479, 0.83304145],\n",
       "       [0.22760061, 0.03372788, 0.79030736, 0.47607227],\n",
       "       ...,\n",
       "       [0.73372942, 0.79926651, 0.18562518, 0.69688571],\n",
       "       [0.66186019, 0.78146495, 0.25258233, 0.83762696],\n",
       "       [0.50811121, 0.90912075, 0.06780537, 0.22092801]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Dataset[:, 0:4]\n",
    "Y = Dataset[:, 4]\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "\n",
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_scale, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shahe\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 548 samples, validate on 548 samples\n",
      "Epoch 1/150\n",
      "548/548 [==============================] - 2s 4ms/step - loss: 0.6684 - acc: 0.6241 - val_loss: 0.6578 - val_acc: 0.5420\n",
      "Epoch 2/150\n",
      "548/548 [==============================] - 0s 72us/step - loss: 0.6368 - acc: 0.5785 - val_loss: 0.6460 - val_acc: 0.5584\n",
      "Epoch 3/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.6187 - acc: 0.6168 - val_loss: 0.6270 - val_acc: 0.6022\n",
      "Epoch 4/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.5969 - acc: 0.6460 - val_loss: 0.6056 - val_acc: 0.6314\n",
      "Epoch 5/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.5712 - acc: 0.7080 - val_loss: 0.5808 - val_acc: 0.7190\n",
      "Epoch 6/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.5448 - acc: 0.7372 - val_loss: 0.5593 - val_acc: 0.7318\n",
      "Epoch 7/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.5173 - acc: 0.7847 - val_loss: 0.5332 - val_acc: 0.7719\n",
      "Epoch 8/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.4919 - acc: 0.8285 - val_loss: 0.5099 - val_acc: 0.7920\n",
      "Epoch 9/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.4634 - acc: 0.8358 - val_loss: 0.4854 - val_acc: 0.8084\n",
      "Epoch 10/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.4382 - acc: 0.8412 - val_loss: 0.4628 - val_acc: 0.8193\n",
      "Epoch 11/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.4150 - acc: 0.8613 - val_loss: 0.4413 - val_acc: 0.8266\n",
      "Epoch 12/150\n",
      "548/548 [==============================] - 0s 71us/step - loss: 0.3944 - acc: 0.8467 - val_loss: 0.4227 - val_acc: 0.8321\n",
      "Epoch 13/150\n",
      "548/548 [==============================] - ETA: 0s - loss: 0.5179 - acc: 0.812 - 0s 58us/step - loss: 0.3741 - acc: 0.8723 - val_loss: 0.4079 - val_acc: 0.8321\n",
      "Epoch 14/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.3588 - acc: 0.8650 - val_loss: 0.3877 - val_acc: 0.8485\n",
      "Epoch 15/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.3401 - acc: 0.8741 - val_loss: 0.3757 - val_acc: 0.8485\n",
      "Epoch 16/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.3234 - acc: 0.8759 - val_loss: 0.3571 - val_acc: 0.8595\n",
      "Epoch 17/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.3121 - acc: 0.8796 - val_loss: 0.3432 - val_acc: 0.8704\n",
      "Epoch 18/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.2995 - acc: 0.8960 - val_loss: 0.3325 - val_acc: 0.8686\n",
      "Epoch 19/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.2885 - acc: 0.8923 - val_loss: 0.3216 - val_acc: 0.8704\n",
      "Epoch 20/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.2768 - acc: 0.8905 - val_loss: 0.3047 - val_acc: 0.8923\n",
      "Epoch 21/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.2621 - acc: 0.9142 - val_loss: 0.2957 - val_acc: 0.8887\n",
      "Epoch 22/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.2539 - acc: 0.9270 - val_loss: 0.2844 - val_acc: 0.8996\n",
      "Epoch 23/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.2488 - acc: 0.9124 - val_loss: 0.2709 - val_acc: 0.9088\n",
      "Epoch 24/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.2323 - acc: 0.9288 - val_loss: 0.2619 - val_acc: 0.9124\n",
      "Epoch 25/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.2242 - acc: 0.9380 - val_loss: 0.2497 - val_acc: 0.9197\n",
      "Epoch 26/150\n",
      "548/548 [==============================] - 0s 75us/step - loss: 0.2146 - acc: 0.9398 - val_loss: 0.2412 - val_acc: 0.9197\n",
      "Epoch 27/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.2069 - acc: 0.9398 - val_loss: 0.2326 - val_acc: 0.9252\n",
      "Epoch 28/150\n",
      "548/548 [==============================] - 0s 91us/step - loss: 0.2000 - acc: 0.9453 - val_loss: 0.2217 - val_acc: 0.9234\n",
      "Epoch 29/150\n",
      "548/548 [==============================] - 0s 87us/step - loss: 0.1917 - acc: 0.9471 - val_loss: 0.2149 - val_acc: 0.9307\n",
      "Epoch 30/150\n",
      "548/548 [==============================] - 0s 88us/step - loss: 0.1850 - acc: 0.9471 - val_loss: 0.2045 - val_acc: 0.9288\n",
      "Epoch 31/150\n",
      "548/548 [==============================] - 0s 89us/step - loss: 0.1772 - acc: 0.9507 - val_loss: 0.1974 - val_acc: 0.9343\n",
      "Epoch 32/150\n",
      "548/548 [==============================] - 0s 73us/step - loss: 0.1694 - acc: 0.9507 - val_loss: 0.1867 - val_acc: 0.9361\n",
      "Epoch 33/150\n",
      "548/548 [==============================] - 0s 54us/step - loss: 0.1625 - acc: 0.9526 - val_loss: 0.1844 - val_acc: 0.9434\n",
      "Epoch 34/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.1584 - acc: 0.9562 - val_loss: 0.1717 - val_acc: 0.9380\n",
      "Epoch 35/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.1477 - acc: 0.9580 - val_loss: 0.1663 - val_acc: 0.9471\n",
      "Epoch 36/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.1422 - acc: 0.9562 - val_loss: 0.1567 - val_acc: 0.9453\n",
      "Epoch 37/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.1363 - acc: 0.9599 - val_loss: 0.1546 - val_acc: 0.9544\n",
      "Epoch 38/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.1362 - acc: 0.9617 - val_loss: 0.1417 - val_acc: 0.9544\n",
      "Epoch 39/150\n",
      "548/548 [==============================] - 0s 61us/step - loss: 0.1238 - acc: 0.9635 - val_loss: 0.1381 - val_acc: 0.9507\n",
      "Epoch 40/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.1175 - acc: 0.9708 - val_loss: 0.1303 - val_acc: 0.9544\n",
      "Epoch 41/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.1134 - acc: 0.9726 - val_loss: 0.1250 - val_acc: 0.9580\n",
      "Epoch 42/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.1093 - acc: 0.9745 - val_loss: 0.1231 - val_acc: 0.9580\n",
      "Epoch 43/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.1077 - acc: 0.9745 - val_loss: 0.1139 - val_acc: 0.9617\n",
      "Epoch 44/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.1067 - acc: 0.9708 - val_loss: 0.1152 - val_acc: 0.9599\n",
      "Epoch 45/150\n",
      "548/548 [==============================] - 0s 67us/step - loss: 0.0989 - acc: 0.9745 - val_loss: 0.1049 - val_acc: 0.9617\n",
      "Epoch 46/150\n",
      "548/548 [==============================] - 0s 73us/step - loss: 0.0929 - acc: 0.9763 - val_loss: 0.1002 - val_acc: 0.9653\n",
      "Epoch 47/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0899 - acc: 0.9781 - val_loss: 0.0989 - val_acc: 0.9672\n",
      "Epoch 48/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0862 - acc: 0.9763 - val_loss: 0.0927 - val_acc: 0.9653\n",
      "Epoch 49/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0824 - acc: 0.9781 - val_loss: 0.0908 - val_acc: 0.9690\n",
      "Epoch 50/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0799 - acc: 0.9799 - val_loss: 0.0856 - val_acc: 0.9763\n",
      "Epoch 51/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0755 - acc: 0.9799 - val_loss: 0.0819 - val_acc: 0.9781\n",
      "Epoch 52/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0728 - acc: 0.9818 - val_loss: 0.0786 - val_acc: 0.9781\n",
      "Epoch 53/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0707 - acc: 0.9818 - val_loss: 0.0757 - val_acc: 0.9836\n",
      "Epoch 54/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0686 - acc: 0.9818 - val_loss: 0.0729 - val_acc: 0.9854\n",
      "Epoch 55/150\n",
      "548/548 [==============================] - 0s 103us/step - loss: 0.0675 - acc: 0.9799 - val_loss: 0.0700 - val_acc: 0.9836\n",
      "Epoch 56/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0640 - acc: 0.9836 - val_loss: 0.0684 - val_acc: 0.9836\n",
      "Epoch 57/150\n",
      "548/548 [==============================] - 0s 75us/step - loss: 0.0656 - acc: 0.9799 - val_loss: 0.0663 - val_acc: 0.9891\n",
      "Epoch 58/150\n",
      "548/548 [==============================] - 0s 69us/step - loss: 0.0594 - acc: 0.9818 - val_loss: 0.0641 - val_acc: 0.9872\n",
      "Epoch 59/150\n",
      "548/548 [==============================] - 0s 65us/step - loss: 0.0664 - acc: 0.9818 - val_loss: 0.0623 - val_acc: 0.9927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0588 - acc: 0.9872 - val_loss: 0.0593 - val_acc: 0.9909\n",
      "Epoch 61/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0535 - acc: 0.9836 - val_loss: 0.0569 - val_acc: 0.9927\n",
      "Epoch 62/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.0519 - acc: 0.9854 - val_loss: 0.0552 - val_acc: 0.9927\n",
      "Epoch 63/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0505 - acc: 0.9836 - val_loss: 0.0536 - val_acc: 0.9927\n",
      "Epoch 64/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0491 - acc: 0.9872 - val_loss: 0.0519 - val_acc: 0.9945\n",
      "Epoch 65/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0478 - acc: 0.9836 - val_loss: 0.0503 - val_acc: 0.9945\n",
      "Epoch 66/150\n",
      "548/548 [==============================] - 0s 67us/step - loss: 0.0462 - acc: 0.9891 - val_loss: 0.0490 - val_acc: 0.9945\n",
      "Epoch 67/150\n",
      "548/548 [==============================] - 0s 73us/step - loss: 0.0451 - acc: 0.9872 - val_loss: 0.0478 - val_acc: 0.9945\n",
      "Epoch 68/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0439 - acc: 0.9854 - val_loss: 0.0472 - val_acc: 0.9945\n",
      "Epoch 69/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0448 - acc: 0.9909 - val_loss: 0.0455 - val_acc: 0.9945\n",
      "Epoch 70/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0427 - acc: 0.9872 - val_loss: 0.0448 - val_acc: 0.9945\n",
      "Epoch 71/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0420 - acc: 0.9891 - val_loss: 0.0428 - val_acc: 0.9945\n",
      "Epoch 72/150\n",
      "548/548 [==============================] - 0s 86us/step - loss: 0.0434 - acc: 0.9927 - val_loss: 0.0418 - val_acc: 0.9945\n",
      "Epoch 73/150\n",
      "548/548 [==============================] - 0s 71us/step - loss: 0.0413 - acc: 0.9872 - val_loss: 0.0412 - val_acc: 0.9945\n",
      "Epoch 74/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0371 - acc: 0.9927 - val_loss: 0.0397 - val_acc: 0.9945\n",
      "Epoch 75/150\n",
      "548/548 [==============================] - 0s 67us/step - loss: 0.0392 - acc: 0.9909 - val_loss: 0.0390 - val_acc: 0.9945\n",
      "Epoch 76/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0367 - acc: 0.9927 - val_loss: 0.0380 - val_acc: 0.9945\n",
      "Epoch 77/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0342 - acc: 0.9927 - val_loss: 0.0370 - val_acc: 0.9945\n",
      "Epoch 78/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0338 - acc: 0.9927 - val_loss: 0.0361 - val_acc: 0.9945\n",
      "Epoch 79/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0332 - acc: 0.9927 - val_loss: 0.0354 - val_acc: 0.9945\n",
      "Epoch 80/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0332 - acc: 0.9927 - val_loss: 0.0350 - val_acc: 0.9945\n",
      "Epoch 81/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0316 - acc: 0.9945 - val_loss: 0.0344 - val_acc: 0.9945\n",
      "Epoch 82/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0311 - acc: 0.9945 - val_loss: 0.0333 - val_acc: 0.9945\n",
      "Epoch 83/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0307 - acc: 0.9927 - val_loss: 0.0326 - val_acc: 0.9945\n",
      "Epoch 84/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0302 - acc: 0.9945 - val_loss: 0.0320 - val_acc: 0.9945\n",
      "Epoch 85/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0305 - acc: 0.9927 - val_loss: 0.0316 - val_acc: 0.9964\n",
      "Epoch 86/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0287 - acc: 0.9964 - val_loss: 0.0317 - val_acc: 0.9945\n",
      "Epoch 87/150\n",
      "548/548 [==============================] - 0s 67us/step - loss: 0.0276 - acc: 0.9945 - val_loss: 0.0300 - val_acc: 0.9945\n",
      "Epoch 88/150\n",
      "548/548 [==============================] - 0s 63us/step - loss: 0.0276 - acc: 0.9964 - val_loss: 0.0295 - val_acc: 0.9964\n",
      "Epoch 89/150\n",
      "548/548 [==============================] - 0s 51us/step - loss: 0.0279 - acc: 0.9945 - val_loss: 0.0288 - val_acc: 0.9964\n",
      "Epoch 90/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0442 - acc: 0.9909 - val_loss: 0.0340 - val_acc: 0.9891\n",
      "Epoch 91/150\n",
      "548/548 [==============================] - 0s 57us/step - loss: 0.0309 - acc: 0.9891 - val_loss: 0.0279 - val_acc: 0.9964\n",
      "Epoch 92/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0250 - acc: 0.9964 - val_loss: 0.0274 - val_acc: 0.9964\n",
      "Epoch 93/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0260 - acc: 0.9964 - val_loss: 0.0276 - val_acc: 0.9964\n",
      "Epoch 94/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0246 - acc: 0.9964 - val_loss: 0.0266 - val_acc: 0.9964\n",
      "Epoch 95/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.0236 - acc: 0.9964 - val_loss: 0.0260 - val_acc: 0.9964\n",
      "Epoch 96/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0231 - acc: 0.9945 - val_loss: 0.0265 - val_acc: 0.9945\n",
      "Epoch 97/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.0250 - val_acc: 0.9964\n",
      "Epoch 98/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0225 - acc: 0.9964 - val_loss: 0.0247 - val_acc: 0.9964\n",
      "Epoch 99/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0220 - acc: 0.9964 - val_loss: 0.0256 - val_acc: 0.9945\n",
      "Epoch 100/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0243 - val_acc: 0.9964\n",
      "Epoch 101/150\n",
      "548/548 [==============================] - 0s 63us/step - loss: 0.0212 - acc: 0.9964 - val_loss: 0.0236 - val_acc: 0.9964\n",
      "Epoch 102/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0209 - acc: 0.9964 - val_loss: 0.0242 - val_acc: 0.9964\n",
      "Epoch 103/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0208 - acc: 0.9964 - val_loss: 0.0229 - val_acc: 0.9964\n",
      "Epoch 104/150\n",
      "548/548 [==============================] - 0s 76us/step - loss: 0.0200 - acc: 0.9964 - val_loss: 0.0235 - val_acc: 0.9964\n",
      "Epoch 105/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0221 - val_acc: 0.9964\n",
      "Epoch 106/150\n",
      "548/548 [==============================] - 0s 75us/step - loss: 0.0214 - acc: 0.9964 - val_loss: 0.0223 - val_acc: 0.9964\n",
      "Epoch 107/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0191 - acc: 0.9964 - val_loss: 0.0227 - val_acc: 0.9964\n",
      "Epoch 108/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0194 - acc: 0.9964 - val_loss: 0.0222 - val_acc: 0.9964\n",
      "Epoch 109/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0186 - acc: 0.9964 - val_loss: 0.0212 - val_acc: 0.9964\n",
      "Epoch 110/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0183 - acc: 0.9964 - val_loss: 0.0207 - val_acc: 0.9964\n",
      "Epoch 111/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0183 - acc: 0.9964 - val_loss: 0.0209 - val_acc: 0.9964\n",
      "Epoch 112/150\n",
      "548/548 [==============================] - 0s 71us/step - loss: 0.0175 - acc: 0.9964 - val_loss: 0.0210 - val_acc: 0.9964\n",
      "Epoch 113/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0176 - acc: 0.9964 - val_loss: 0.0202 - val_acc: 0.9964\n",
      "Epoch 114/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0174 - acc: 0.9964 - val_loss: 0.0196 - val_acc: 0.9964\n",
      "Epoch 115/150\n",
      "548/548 [==============================] - 0s 131us/step - loss: 0.0168 - acc: 0.9964 - val_loss: 0.0196 - val_acc: 0.9964\n",
      "Epoch 116/150\n",
      "548/548 [==============================] - 0s 73us/step - loss: 0.0164 - acc: 0.9964 - val_loss: 0.0190 - val_acc: 0.9964\n",
      "Epoch 117/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0163 - acc: 0.9964 - val_loss: 0.0189 - val_acc: 0.9964\n",
      "Epoch 118/150\n",
      "548/548 [==============================] - 0s 73us/step - loss: 0.0162 - acc: 0.9964 - val_loss: 0.0186 - val_acc: 0.9964\n",
      "Epoch 119/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0173 - acc: 0.9964 - val_loss: 0.0186 - val_acc: 0.9964\n",
      "Epoch 120/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548/548 [==============================] - 0s 60us/step - loss: 0.0164 - acc: 0.9964 - val_loss: 0.0180 - val_acc: 0.9964\n",
      "Epoch 121/150\n",
      "548/548 [==============================] - 0s 67us/step - loss: 0.0158 - acc: 0.9964 - val_loss: 0.0179 - val_acc: 0.9964\n",
      "Epoch 122/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0152 - acc: 0.9964 - val_loss: 0.0175 - val_acc: 0.9964\n",
      "Epoch 123/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0146 - acc: 0.9964 - val_loss: 0.0172 - val_acc: 0.9964\n",
      "Epoch 124/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0145 - acc: 0.9964 - val_loss: 0.0175 - val_acc: 0.9964\n",
      "Epoch 125/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0144 - acc: 0.9964 - val_loss: 0.0169 - val_acc: 0.9964\n",
      "Epoch 126/150\n",
      "548/548 [==============================] - 0s 62us/step - loss: 0.0144 - acc: 0.9964 - val_loss: 0.0166 - val_acc: 0.9964\n",
      "Epoch 127/150\n",
      "548/548 [==============================] - 0s 76us/step - loss: 0.0138 - acc: 0.9964 - val_loss: 0.0163 - val_acc: 0.9964\n",
      "Epoch 128/150\n",
      "548/548 [==============================] - 0s 71us/step - loss: 0.0146 - acc: 0.9964 - val_loss: 0.0168 - val_acc: 0.9964\n",
      "Epoch 129/150\n",
      "548/548 [==============================] - 0s 71us/step - loss: 0.0137 - acc: 0.9964 - val_loss: 0.0161 - val_acc: 0.9964\n",
      "Epoch 130/150\n",
      "548/548 [==============================] - 0s 58us/step - loss: 0.0137 - acc: 0.9964 - val_loss: 0.0161 - val_acc: 0.9964\n",
      "Epoch 131/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0131 - acc: 0.9964 - val_loss: 0.0159 - val_acc: 0.9964\n",
      "Epoch 132/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0134 - acc: 0.9964 - val_loss: 0.0150 - val_acc: 0.9982\n",
      "Epoch 133/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0214 - acc: 0.9982 - val_loss: 0.0182 - val_acc: 0.9964\n",
      "Epoch 134/150\n",
      "548/548 [==============================] - 0s 64us/step - loss: 0.0149 - acc: 0.9964 - val_loss: 0.0152 - val_acc: 0.9964\n",
      "Epoch 135/150\n",
      "548/548 [==============================] - 0s 67us/step - loss: 0.0118 - acc: 0.9982 - val_loss: 0.0156 - val_acc: 0.9964\n",
      "Epoch 136/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0126 - acc: 0.9964 - val_loss: 0.0165 - val_acc: 0.9964\n",
      "Epoch 137/150\n",
      "548/548 [==============================] - 0s 66us/step - loss: 0.0119 - acc: 0.9964 - val_loss: 0.0148 - val_acc: 0.9964\n",
      "Epoch 138/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0140 - val_acc: 0.9982\n",
      "Epoch 139/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0142 - val_acc: 0.9982\n",
      "Epoch 140/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0142 - val_acc: 0.9982\n",
      "Epoch 141/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0108 - acc: 0.9982 - val_loss: 0.0147 - val_acc: 0.9964\n",
      "Epoch 142/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0141 - val_acc: 0.9964\n",
      "Epoch 143/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0141 - val_acc: 0.9982\n",
      "Epoch 144/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0104 - acc: 0.9964 - val_loss: 0.0131 - val_acc: 0.9982\n",
      "Epoch 145/150\n",
      "548/548 [==============================] - 0s 53us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0134 - val_acc: 0.9982\n",
      "Epoch 146/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0134 - val_acc: 0.9982\n",
      "Epoch 147/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0101 - acc: 0.9982 - val_loss: 0.0144 - val_acc: 0.9964\n",
      "Epoch 148/150\n",
      "548/548 [==============================] - 0s 56us/step - loss: 0.0098 - acc: 0.9982 - val_loss: 0.0123 - val_acc: 0.9982\n",
      "Epoch 149/150\n",
      "548/548 [==============================] - 0s 55us/step - loss: 0.0099 - acc: 0.9964 - val_loss: 0.0125 - val_acc: 0.9982\n",
      "Epoch 150/150\n",
      "548/548 [==============================] - 0s 60us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9982\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential([    Dense(32, activation='relu', input_shape=(4,)),    Dense(32, activation='relu'),    Dense(1, activation='sigmoid'),])\n",
    "model.compile(optimizer='adam',              loss='binary_crossentropy',              metrics=['accuracy'])\n",
    "hist = model.fit(X_train, Y_train,          batch_size=32, epochs=150,          validation_data=(X_val, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548/548 [==============================] - 0s 24us/step\n",
      "Accuracy: 99.82\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_val, Y_val)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTEST = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.round(YTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
